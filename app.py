import streamlit as st
import folium
from streamlit_folium import st_folium
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta, timezone
import re
import random
import time

# --- è¨­å®š ---
CACHE_TTL = 600 # 10åˆ†

st.set_page_config(page_title="ç§‹ç”°çœŒè¿‘è¾ºã‚¹ã‚­ãƒ¼å ´æƒ…å ±", layout="wide")

# --- æ™‚é–“è¨­å®š (JST) ---
JST = timezone(timedelta(hours=9), 'JST')
now_jst = datetime.now(timezone.utc).astimezone(JST)
ACCESS_TIME = now_jst.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
today_str = now_jst.strftime("%m/%d")

# --- CSS ---
st.markdown("""
<style>
    .table-container { max-height: 700px; overflow: auto; border: 1px solid #ddd; margin-bottom: 30px; }
    table { width: 100%; border-collapse: collapse; font-family: sans-serif; font-size: 13px; white-space: nowrap; }
    th, td { padding: 10px 12px; text-align: left; border-bottom: 1px solid #ddd; }
    thead th { position: sticky; top: 0; background-color: #008CBA; color: white; z-index: 2; }
    th:first-child, td:first-child { position: sticky; left: 0; background-color: #008CBA; z-index: 3; }
    tbody td:first-child { background-color: #fff; z-index: 1; font-weight: bold; border-right: 2px solid #ddd; }
    tbody tr:nth-child(even) { background-color: #f8f9fa; }
    tbody tr:nth-child(even) td:first-child { background-color: #f8f9fa; }
    
    .status-ok { color: green; font-weight: bold; background:#e6fffa; padding:2px 5px; border-radius:4px; }
    .status-ng { color: #d9534f; font-weight: bold; background:#fff5f5; padding:2px 5px; border-radius:4px; }
    .status-warn { color: #856404; font-weight: bold; background:#fff3cd; padding:2px 5px; border-radius:4px; }
    .no-data { color: #aaa; font-style:italic; font-size: 0.9em; }
    .link-btn { background: #fff; border: 1px solid #008CBA; color: #008CBA; padding: 2px 8px; border-radius: 4px; text-decoration: none; font-size: 0.8em;}
    .update-info { background:#d1e7dd; color:#0f5132; padding:10px; border-radius:5px; margin-bottom:15px; font-size:0.9em; border:1px solid #badbcc;}
</style>
""", unsafe_allow_html=True)

st.title("â›·ï¸ ç§‹ç”°çœŒè¿‘è¾ºã‚¹ã‚­ãƒ¼å ´ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±é›†ç´„")
st.markdown(f"##### ç‰¹åŒ–å‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè£…ç‰ˆ")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
filter_open_only = st.sidebar.checkbox("å–¶æ¥­ä¸­ã®ã¿è¡¨ç¤º", value=False)

# --- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»ãƒ˜ãƒ«ãƒ‘ãƒ¼ ---
def get_random_headers():
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0"
    ]
    return {
        "User-Agent": random.choice(user_agents),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Cache-Control": "max-age=0",
        "Upgrade-Insecure-Requests": "1"
    }

def extract_number(text):
    if not text: return None
    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')).strip()
    match = re.search(r'(\d{1,3})\s*(cm|ã)?', text, re.IGNORECASE)
    if match:
        val = int(match.group(1))
        if 0 <= val <= 600: return val
    return None

def find_status_in_text(text):
    if "å…¨é¢æ»‘èµ°å¯" in text or "å…¨é¢å¯" in text: return "âœ… å…¨é¢å¯"
    if "ä¸€éƒ¨æ»‘èµ°" in text or "ä¸€éƒ¨å¯" in text: return "âš ï¸ ä¸€éƒ¨å¯"
    if "å–¶æ¥­ä¸­" in text: return "âœ… å–¶æ¥­ä¸­"
    if "æº–å‚™ä¸­" in text: return "â›” æº–å‚™ä¸­"
    if "ã‚¯ãƒ­ãƒ¼ã‚º" in text or "çµ‚äº†" in text or "é‹ä¼‘" in text or "ä¼‘æ¥­" in text: return "â›” ã‚¯ãƒ­ãƒ¼ã‚º"
    return None

# --- å€‹åˆ¥å¯¾å¿œãƒ­ã‚¸ãƒƒã‚¯ ---
def scrape_geto_kogen(soup, text_body):
    """å¤æ²¹é«˜åŸ å°‚ç”¨ãƒ­ã‚¸ãƒƒã‚¯"""
    # å¤æ²¹ã¯ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã€ã¾ãŸã¯ç‰¹å®šã®ã‚¯ãƒ©ã‚¹åã§æŒã£ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„
    # ä¾‹: <td class="snow">150cm</td> ãªã©ã‚’æƒ³å®š
    
    snow = None
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "ç©é›ª"ã®ã‚»ãƒ«ã‚’æ¢ã—ã€ãã®éš£ã®ã‚»ãƒ«ã‚’å–å¾—
    targets = soup.find_all(['th', 'td'], string=re.compile(r'ç©é›ª'))
    for t in targets:
        sibling = t.find_next_sibling(['td', 'th'])
        if sibling:
            val = extract_number(sibling.get_text())
            if val: 
                snow = val
                break
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ "ç©é›ª" ã®ç›´å¾Œã®æ•°å­—ã‚’æ¢ã™ (å¼·åŠ›ãªæ­£è¦è¡¨ç¾)
    if not snow:
        match = re.search(r'ç©é›ª.*?(\d{1,3})\s*cm', text_body, re.DOTALL)
        if match: snow = int(match.group(1))

    return snow

def scrape_hachimantai(soup, text_body):
    """ç§‹ç”°å…«å¹¡å¹³ å°‚ç”¨ãƒ­ã‚¸ãƒƒã‚¯"""
    snow = None
    # å…«å¹¡å¹³ã¯ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã® "æœ¬æ—¥ã®ã‚²ãƒ¬ãƒ³ãƒ‡æƒ…å ±" ãªã©ã®æ å†…ã«ã‚ã‚‹ã“ã¨ãŒå¤šã„
    # "ç©é›ª" ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¿‘ãã‚’æ¢ç´¢
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: dt/ddæ§‹é€ 
    targets = soup.find_all('dt', string=re.compile(r'ç©é›ª'))
    for t in targets:
        dd = t.find_next_sibling('dd')
        if dd:
            val = extract_number(dd.get_text())
            if val:
                snow = val
                break
                
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
    if not snow:
        match = re.search(r'ç©é›ª.*?(\d{1,3})\s*cm', text_body)
        if match: snow = int(match.group(1))
        
    return snow

# --- ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° ---
@st.cache_data(ttl=CACHE_TTL)
def scrape_resort(url, name, total_courses):
    data = {"snow": "æœªå–å¾—", "status": "ç¢ºèªä¸­", "open_count": "?"}
    
    try:
        res = requests.get(url, headers=get_random_headers(), timeout=10)
        res.encoding = res.apparent_encoding
        
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, 'html.parser')
            text_body = soup.get_text(separator=' ', strip=True)
            
            # --- ç©é›ªæƒ…å ±ã®å–å¾— (å€‹åˆ¥å¯¾å¿œ or æ±ç”¨) ---
            snow_val = None
            
            if "å¤æ²¹" in name:
                snow_val = scrape_geto_kogen(soup, text_body)
            elif "å…«å¹¡å¹³" in name:
                snow_val = scrape_hachimantai(soup, text_body)
            
            # å€‹åˆ¥å¯¾å¿œã§å–ã‚Œãªã‹ã£ãŸã€ã¾ãŸã¯ãã®ä»–ã®ã‚¹ã‚­ãƒ¼å ´ã¯æ±ç”¨ãƒ­ã‚¸ãƒƒã‚¯
            if not snow_val:
                # æ±ç”¨: "ç©é›ª"ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã®æ•°å­—ã‚’æ¢ã™
                keywords = ["ç©é›ª", "å±±é ‚", "SNOW DEPTH", "Snow"]
                for key in keywords:
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¦ç´ ã‚’ç‰¹å®š
                    elements = soup.find_all(string=re.compile(key))
                    for el in elements:
                        # ãã®è¦ç´ ã®è¦ªã‚„éš£æ¥è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã¦æ•°å­—ã‚’æ¢ã™
                        parent_text = el.parent.get_text()
                        val = extract_number(parent_text)
                        if val:
                            snow_val = val
                            break
                        # è¦ªã®è¦ªã¾ã§é¡ã‚‹ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ãªã©ï¼‰
                        grandparent_text = el.parent.parent.get_text()
                        val = extract_number(grandparent_text)
                        if val:
                            snow_val = val
                            break
                    if snow_val: break

            if snow_val:
                data["snow"] = f"{snow_val}cm"
            else:
                data["snow"] = "è¨˜è¼‰ãªã—"

            # --- å–¶æ¥­çŠ¶æ³ã®åˆ¤å®š ---
            status = find_status_in_text(text_body)
            if status:
                data["status"] = status
            else:
                data["status"] = "ä¸æ˜"

            # --- ã‚³ãƒ¼ã‚¹æ•° ---
            if "å…¨é¢" in data["status"]:
                data["open_count"] = total_courses
            elif "ã‚¯ãƒ­ãƒ¼ã‚º" in data["status"] or "æº–å‚™" in data["status"]:
                data["open_count"] = 0
            else:
                match_c = re.search(r'(\d{1,2})\s*(ã‚³ãƒ¼ã‚¹|æœ¬).*?(æ»‘èµ°|ã‚ªãƒ¼ãƒ—ãƒ³|å¯)', text_body)
                if match_c:
                    data["open_count"] = int(match_c.group(1))
        else:
            data["status"] = f"ã‚¨ãƒ©ãƒ¼({res.status_code})"
            
    except Exception:
        pass # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆæœŸå€¤ã®ã¾ã¾
        
    return data

# --- ãƒ‡ãƒ¼ã‚¿å®šç¾© ---
base_resorts = [
    {
        "name": "å¤æ²¹é«˜åŸ", "full_name": "å¤æ²¹é«˜åŸã‚¹ã‚­ãƒ¼å ´", "url": "https://www.getokogen.com/", 
        "lat": 39.2178, "lon": 140.9242, "time": 115, "dist": 139, "price": 6800,
        "total": 14, "groom": 10, "ungroom": 4, 
        "yt_id": "Vo9xtIyktUY", "live": "https://www.youtube.com/@getokogen/live"
    },
    {
        "name": "ç§‹ç”°å…«å¹¡å¹³", "full_name": "ç§‹ç”°å…«å¹¡å¹³ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.akihachi.jp/", 
        "lat": 39.9922, "lon": 140.8358, "time": 115, "dist": 105, "price": 4000,
        "total": 4, "groom": 2, "ungroom": 2, 
        "live": "https://www.akihachi.jp/"
    },
    {
        "name": "é˜¿ä»", "full_name": "æ£®å‰å±±é˜¿ä»ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.aniski.jp/", 
        "lat": 39.9575, "lon": 140.4564, "time": 85, "dist": 79, "price": 4500,
        "total": 5, "groom": 3, "ungroom": 2, 
        "live": "https://www.aniski.jp/livecam/"
    },
    {
        "name": "ãŸã–ã‚æ¹–", "full_name": "ãŸã–ã‚æ¹–ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.tazawako-ski.com/", 
        "lat": 39.7567, "lon": 140.7811, "time": 90, "dist": 78, "price": 5300,
        "total": 13, "groom": 9, "ungroom": 4, 
        "live": "http://www.tazawako-ski.com/gelande/"
    },
    {
        "name": "é›«çŸ³", "full_name": "é›«çŸ³ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.princehotels.co.jp/ski/shizukuishi/", 
        "lat": 39.6953, "lon": 140.9736, "time": 100, "dist": 90, "price": 6200,
        "total": 11, "groom": 9, "ungroom": 2, 
        "live": "https://www.princehotels.co.jp/ski/shizukuishi/"
    },
    {
        "name": "ã‚ªãƒ¼ãƒ‘ã‚¹", "full_name": "å¤ªå¹³å±±ã‚¹ã‚­ãƒ¼å ´ã‚ªãƒ¼ãƒ‘ã‚¹", "url": "http://www.theboon.net/opas/", 
        "lat": 39.7894, "lon": 140.1983, "time": 30, "dist": 22, "price": 2200,
        "total": 5, "groom": 5, "ungroom": 0, 
        "live": "http://www.theboon.net/opas/livecam.html"
    },
    {
        "name": "ã‚¸ãƒ¥ãƒã‚¹æ —é§’", "full_name": "ã‚¸ãƒ¥ãƒã‚¹æ —é§’ã‚¹ã‚­ãƒ¼å ´", "url": "https://jeunesse-ski.com/", 
        "lat": 39.1950, "lon": 140.6922, "time": 95, "dist": 110, "price": 4000,
        "total": 12, "groom": 10, "ungroom": 2, 
        "live": "https://jeunesse-ski.com/live-camera/"
    },
    {
        "name": "çŸ¢å³¶", "full_name": "é³¥æµ·é«˜åŸçŸ¢å³¶ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.yashimaski.com/", 
        "lat": 39.1866, "lon": 140.1264, "time": 85, "dist": 91, "price": 3000,
        "total": 6, "groom": 5, "ungroom": 1, 
        "live": "https://ski.city.yurihonjo.lg.jp/live-camera/"
    },
    {
        "name": "å”å’Œ", "full_name": "å”å’Œã‚¹ã‚­ãƒ¼å ´", "url": "https://kyowasnow.net/", 
        "lat": 39.6384, "lon": 140.3230, "time": 50, "dist": 45, "price": 3300,
        "total": 7, "groom": 7, "ungroom": 0, 
        "live": "https://kyowasnow.net/"
    },
    {
        "name": "èŠ±è¼ª", "full_name": "èŠ±è¼ªã‚¹ã‚­ãƒ¼å ´", "url": "https://www.alpas.jp/", 
        "lat": 40.1833, "lon": 140.7871, "time": 115, "dist": 112, "price": 3400,
        "total": 7, "groom": 7, "ungroom": 0, 
    },
    {
        "name": "æ°´æ™¶å±±", "full_name": "æ°´æ™¶å±±ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.city.shizukuishi.iwate.jp/", 
        "lat": 39.7344, "lon": 140.6275, "time": 90, "dist": 88, "price": 3000,
        "total": 4, "groom": 4, "ungroom": 0, 
    },
    {
        "name": "å¤§å°", "full_name": "å¤§å°ã‚¹ã‚­ãƒ¼å ´", "url": "https://ohdai.omagari-sc.com/", 
        "lat": 39.4625, "lon": 140.5592, "time": 60, "dist": 65, "price": 3100,
        "total": 6, "groom": 6, "ungroom": 0, 
    },
    {
        "name": "å¤©ä¸‹æ£®", "full_name": "å¤©ä¸‹æ£®ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.city.yokote.lg.jp/kanko/1004655/1004664/1001402.html", 
        "lat": 39.2775, "lon": 140.5986, "time": 85, "dist": 95, "price": 2700,
        "total": 2, "groom": 2, "ungroom": 0, 
    },
    {
        "name": "å¤§æ›²ãƒ•ã‚¡ãƒŸãƒªãƒ¼", "full_name": "å¤§æ›²ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.city.daisen.lg.jp/docs/2013110300234/", 
        "lat": 39.4283, "lon": 140.5231, "time": 55, "dist": 60, "price": 2400,
        "total": 1, "groom": 1, "ungroom": 0, 
    },
    {
        "name": "ç¨²å·", "full_name": "ç¨²å·ã‚¹ã‚­ãƒ¼å ´", "url": "https://www.city-yuzawa.jp/site/inakawaski/", 
        "lat": 39.0681, "lon": 140.5894, "time": 95, "dist": 105, "price": 2500,
        "total": 2, "groom": 2, "ungroom": 0, 
    }
]

# --- API (å¤©æ°—) ---
@st.cache_data(ttl=3600)
def get_weather():
    res = {}
    for r in base_resorts:
        try:
            url = "https://api.open-meteo.com/v1/forecast"
            p = {"latitude": r["lat"], "longitude": r["lon"], "daily": "weathercode", "timezone": "Asia/Tokyo", "forecast_days": 2}
            d = requests.get(url, params=p, timeout=2).json()
            c1, c2 = d['daily']['weathercode'][0], d['daily']['weathercode'][1]
            w_map = {0:"â˜€ï¸", 1:"ğŸŒ¤ï¸", 2:"â˜ï¸", 3:"â˜ï¸", 45:"ğŸŒ«ï¸", 51:"ğŸŒ§ï¸", 53:"ğŸŒ§ï¸", 55:"ğŸŒ§ï¸", 61:"â˜”", 63:"â˜”", 71:"â˜ƒï¸", 73:"â˜ƒï¸", 75:"â˜ƒï¸", 77:"ğŸŒ¨ï¸", 80:"ğŸŒ¦ï¸", 85:"ğŸŒ¨ï¸", 95:"âš¡"}
            res[r["name"]] = {"t": w_map.get(c1, "-"), "tm": w_map.get(c2, "-")}
        except:
            res[r["name"]] = {"t": "-", "tm": "-"}
    return res

def fmt_time(m):
    return f"{m//60}æ™‚é–“{m%60}åˆ†" if m//60 > 0 else f"{m}åˆ†"

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
st.markdown(f"""
<div class="update-info">
    <b>ğŸ”„ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­ ({ACCESS_TIME})</b><br>
    å¤æ²¹é«˜åŸãƒ»å…«å¹¡å¹³ãªã©ã¯å°‚ç”¨ãƒ­ã‚¸ãƒƒã‚¯ã§è§£æä¸­ã§ã™ã€‚ãã®ä»–ã¯æ±ç”¨è§£æã‚’è©¦è¡Œã—ã¦ã„ã¾ã™ã€‚
</div>
""", unsafe_allow_html=True)

progress_bar = st.progress(0, text="ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹...")

# 1. å¤©æ°—
weather = get_weather()
progress_bar.progress(10, text="å¤©æ°—å–å¾—å®Œäº†")

# 2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & çµåˆ
df_list = []
cams = []
count = 0
total = len(base_resorts)

for i, r in enumerate(base_resorts):
    progress_bar.progress(10 + int((i/total)*90), text=f"{r['name']} ã‚µã‚¤ãƒˆè§£æä¸­...")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (ãƒªã‚¾ãƒ¼ãƒˆåã‚‚æ¸¡ã—ã¦å€‹åˆ¥å¯¾å¿œ)
    scraped = scrape_resort(r['url'], r['name'], r['total'])
    
    is_open = "å–¶æ¥­" in scraped["status"] or "å¯" in scraped["status"]
    if filter_open_only and not is_open:
        continue
    
    count += 1
    w = weather.get(r["name"], {"t":"-", "tm":"-"})
    t_winter = int(r["time"] * 1.35)
    
    # è¡¨ç¤ºåŠ å·¥
    status_html = scraped['status']
    if "â›”" in status_html or "ã‚¨ãƒ©ãƒ¼" in status_html: 
        status_html = f'<span class="status-ng">{status_html}</span>'
    elif "ç¢ºèªä¸­" in status_html or "ä¸æ˜" in status_html:
        status_html = f'<span class="status-warn">{status_html}</span>'
    else: 
        status_html = f'<span class="status-ok">{status_html}</span>'
    
    snow_val = scraped['snow']
    if snow_val == "æœªå–å¾—" or snow_val == "è¨˜è¼‰ãªã—": snow_val = '<span class="no-data">-</span>'
    else: snow_val = f"<b>{snow_val}</b>"

    open_val = scraped['open_count']
    if open_val == "?": open_val = '<span class="no-data">?</span>'
    course_disp = f"<b>{open_val}</b> / {r['total']}"

    short_name = r["name"]
    if "ã‚ªãƒ¼ãƒ‘ã‚¹" in short_name: short_name = "ã‚ªãƒ¼ãƒ‘ã‚¹"
    else: short_name = short_name.replace("ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚¹ã‚­ãƒ¼å ´", "ãƒ•ã‚¡ãƒŸãƒªãƒ¼").replace("ã‚¹ã‚­ãƒ¼å ´", "")

    df_list.append({
        "ã‚¹ã‚­ãƒ¼å ´": short_name,
        "ã‚ªãƒ¼ãƒ—ãƒ³": r.get("open_date_str", "-"),
        "ç©é›ª": snow_val,
        "çŠ¶æ³": status_html,
        "ã‚³ãƒ¼ã‚¹æ•°<br><span style='font-size:0.8em'>(é–‹/å…¨)</span>": course_disp,
        "å†…è¨³<br><span style='font-size:0.8em'>(åœ§é›ª/éåœ§é›ª)</span>": f"{r['groom']} / {r['ungroom']}",
        "ãƒªãƒ•ãƒˆåˆ¸": f"Â¥{r['price']:,}",
        f"å¤©æ°—({today_str})": w['t'],
        "è·é›¢/æ™‚é–“": f"{r['dist']}km/{fmt_time(t_winter)}",
        "ãƒªãƒ³ã‚¯": f'<a href="{r["url"]}" target="_blank" class="link-btn">å…¬å¼HP</a>',
        "lat": r["lat"], "lon": r["lon"], "raw_status": scraped['status'], "full_name": r["full_name"]
    })
    
    if r.get("live"):
        c = r.copy()
        c["status"] = scraped["status"]
        cams.append(c)

progress_bar.empty()

if count == 0:
    st.error("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ã‚¹ã‚­ãƒ¼å ´ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    df = pd.DataFrame(df_list)
    
    st.subheader("ğŸ“‹ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŠ¶æ³ä¸€è¦§")
    html = df.drop(columns=["lat", "lon", "raw_status", "full_name"]).to_html(classes="table", escape=False, index=False)
    st.markdown(f'<div class="table-container">{html}</div>', unsafe_allow_html=True)
    
    st.divider()
    st.subheader("ğŸ“· ãƒ©ã‚¤ãƒ–ã‚«ãƒ¡ãƒ©")
    cols_per_row = 3
    rows = [cams[i:i + cols_per_row] for i in range(0, len(cams), cols_per_row)]
    for row in rows:
        cols = st.columns(cols_per_row)
        for idx, cam in enumerate(row):
            with cols[idx]:
                if cam.get("yt_id"):
                    thumb = f"https://img.youtube.com/vi/{cam['yt_id']}/mqdefault.jpg"
                else:
                    bg = "008CBA" if "å–¶æ¥­" in cam['status'] or "å¯" in cam['status'] else "6c757d"
                    safe_name = cam['name'].replace(" ", "")
                    thumb = f"https://placehold.co/600x338/{bg}/FFFFFF/png?text={safe_name}"
                st.markdown(f"**{cam['name']}**")
                st.markdown(f"[![cam]({thumb})]({cam['live']})")
    
    st.divider()
    st.subheader("ğŸ—ºï¸ ãƒãƒƒãƒ—")
    m = folium.Map(location=[39.8, 140.5], zoom_start=9)
    for _, row in df.iterrows():
        c = "red" if "å–¶æ¥­" in row['raw_status'] or "å¯" in row['raw_status'] else "blue"
        folium.Marker(
            [row['lat'], row['lon']], popup=row['full_name'], icon=folium.Icon(color=c, icon="info-sign")
        ).add_to(m)
    st_folium(m, width="100%", height=500)
