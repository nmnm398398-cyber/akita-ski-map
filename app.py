import streamlit as st
import folium
from streamlit_folium import st_folium
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta, timezone
import re

# --- è¨­å®š ---
# ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æœŸé™ (1æ™‚é–“)
CACHE_TTL = 3600 

st.set_page_config(page_title="ç§‹ç”°çœŒè¿‘è¾ºã‚¹ã‚­ãƒ¼å ´æƒ…å ±", layout="wide")

# --- æ—¥æ™‚è¨­å®š (JST) ---
JST = timezone(timedelta(hours=9), 'JST')
now_jst = datetime.now(timezone.utc).astimezone(JST)
ACCESS_TIME = now_jst.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

today = now_jst
str_today = today.strftime("%m/%d")
str_tmrw = (today + timedelta(days=1)).strftime("%m/%d")

# --- CSS ---
st.markdown("""
<style>
    .table-container { max-height: 600px; overflow: auto; border: 1px solid #ddd; margin-bottom: 30px; }
    table { width: 100%; border-collapse: collapse; font-family: sans-serif; font-size: 14px; white-space: nowrap; }
    th, td { padding: 12px 15px; text-align: left; border-bottom: 1px solid #ddd; }
    thead th { position: sticky; top: 0; background-color: #008CBA; color: white; z-index: 2; }
    tbody tr:nth-child(even) { background-color: #f8f9fa; }
    .status-ok { color: green; font-weight: bold; }
    .status-ng { color: #d9534f; font-weight: bold; }
    .no-data { color: #999; font-style: italic; }
    .link-btn { background: #fff; border: 1px solid #008CBA; color: #008CBA; padding: 2px 8px; border-radius: 4px; text-decoration: none; font-size: 0.8em;}
</style>
""", unsafe_allow_html=True)

st.title("â›·ï¸ ç§‹ç”°çœŒè¿‘è¾ºã‚¹ã‚­ãƒ¼å ´ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±é›†ç´„")
st.markdown(f"##### è‡ªå‹•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç‰ˆ (ç¾åœ¨æ™‚åˆ»: {ACCESS_TIME})")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
filter_open_only = st.sidebar.checkbox("å–¶æ¥­ä¸­ã¨åˆ¤å®šã•ã‚ŒãŸå ´æ‰€ã®ã¿è¡¨ç¤º", value=False)

# --- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–¢æ•° (æ±ç”¨å‹) ---
@st.cache_data(ttl=CACHE_TTL)
def scrape_resort(url, name):
    """
    æŒ‡å®šURLã‹ã‚‰ç©é›ªæƒ…å ±ãªã©ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡ºã™ã‚‹æ±ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
    """
    data = {
        "snow": "æœªå–å¾—",
        "status": "æœªå–å¾—",
        "raw_text": "" # ãƒ‡ãƒãƒƒã‚°ç”¨
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ5ç§’ã§ã‚¢ã‚¯ã‚»ã‚¹
        res = requests.get(url, headers=headers, timeout=5)
        res.encoding = res.apparent_encoding
        
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, 'html.parser')
            # ç©ºç™½é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            text = soup.get_text().replace('\n', '').replace('\r', '').replace('\t', '').replace(' ', '')
            data["raw_text"] = text[:200] # ãƒ­ã‚°ç”¨
            
            # --- 1. ç©é›ªæ·±ã®æŠ½å‡º (æ­£è¦è¡¨ç¾) ---
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: "ç©é›ª"ã®å¾Œã«ç¶šãæ•°å­— + "cm"
            # ä¾‹: ç©é›ª120cm, ç©é›ª:120cm, å±±é ‚120cm ãªã©
            snow_patterns = [
                r'ç©é›ª[:ï¼š]*([0-9]{1,3})cm',
                r'å±±é ‚[:ï¼š]*([0-9]{1,3})cm',
                r'å±±éº“[:ï¼š]*([0-9]{1,3})cm',
                r'ç©é›ªé‡[:ï¼š]*([0-9]{1,3})cm'
            ]
            
            for pattern in snow_patterns:
                match = re.search(pattern, text)
                if match:
                    data["snow"] = f"{match.group(1)}cm"
                    break # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’æ¡ç”¨
            
            # --- 2. å–¶æ¥­çŠ¶æ³ã®åˆ¤å®š ---
            if "å…¨é¢æ»‘èµ°å¯" in text: data["status"] = "âœ… å…¨é¢å¯"
            elif "ä¸€éƒ¨æ»‘èµ°å¯" in text: data["status"] = "âš ï¸ ä¸€éƒ¨å¯"
            elif "å–¶æ¥­ä¸­" in text: data["status"] = "âœ… å–¶æ¥­ä¸­"
            elif "æº–å‚™ä¸­" in text: data["status"] = "â›” æº–å‚™ä¸­"
            elif "ã‚¯ãƒ­ãƒ¼ã‚º" in text or "ä¼‘æ¥­" in text or "çµ‚äº†" in text: data["status"] = "â›” ã‚¯ãƒ­ãƒ¼ã‚º"
            else:
                # åˆ¤å®šã§ããªã„å ´åˆ
                data["status"] = "ä¸æ˜"

    except Exception:
        pass # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆæœŸå€¤ã€Œæœªå–å¾—ã€ã®ã¾ã¾
        
    return data

# --- ãƒ‡ãƒ¼ã‚¿å®šç¾© (å›ºå®šãƒ‡ãƒ¼ã‚¿ã®ã¿) ---
# â€»ç©é›ªã‚„å–¶æ¥­çŠ¶æ³ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã¯å‰Šé™¤ã—ã¾ã—ãŸ
base_resorts = [
    {"name": "å¤æ²¹é«˜åŸ", "url": "https://www.getokogen.com/", "lat": 39.2178, "lon": 140.9242, "time": 115, "dist": 139, "price": 6800, "yt_id": "Vo9xtIyktUY", "live": "https://www.youtube.com/@getokogen/live"},
    {"name": "ç§‹ç”°å…«å¹¡å¹³", "url": "https://www.akihachi.jp/", "lat": 39.9922, "lon": 140.8358, "time": 115, "dist": 105, "price": 4000, "live": "https://www.akihachi.jp/"},
    {"name": "é˜¿ä»", "url": "https://www.aniski.jp/", "lat": 39.9575, "lon": 140.4564, "time": 85, "dist": 79, "price": 4500, "live": "https://www.aniski.jp/livecam/"},
    {"name": "ãŸã–ã‚æ¹–", "url": "https://www.tazawako-ski.com/", "lat": 39.7567, "lon": 140.7811, "time": 90, "dist": 78, "price": 5300, "live": "http://www.tazawako-ski.com/gelande/"},
    {"name": "ã‚ªãƒ¼ãƒ‘ã‚¹", "url": "http://www.theboon.net/opas/", "lat": 39.7894, "lon": 140.1983, "time": 30, "dist": 22, "price": 2200, "live": "http://www.theboon.net/opas/livecam.html"},
    {"name": "ã‚¸ãƒ¥ãƒã‚¹æ —é§’", "url": "https://jeunesse-ski.com/", "lat": 39.1950, "lon": 140.6922, "time": 95, "dist": 110, "price": 4000, "live": "https://jeunesse-ski.com/live-camera/"},
    {"name": "çŸ¢å³¶", "url": "https://www.yashimaski.com/", "lat": 39.1866, "lon": 140.1264, "time": 85, "dist": 91, "price": 3000, "live": "https://ski.city.yurihonjo.lg.jp/live-camera/"},
    {"name": "å”å’Œ", "url": "https://kyowasnow.net/", "lat": 39.6384, "lon": 140.3230, "time": 50, "dist": 45, "price": 3300, "live": "https://kyowasnow.net/"},
    {"name": "èŠ±è¼ª", "url": "https://www.alpas.jp/", "lat": 40.1833, "lon": 140.7871, "time": 115, "dist": 112, "price": 3400},
    {"name": "æ°´æ™¶å±±", "url": "https://www.city.shizukuishi.iwate.jp/", "lat": 39.7344, "lon": 140.6275, "time": 90, "dist": 88, "price": 3000},
    {"name": "å¤§å°", "url": "https://ohdai.omagari-sc.com/", "lat": 39.4625, "lon": 140.5592, "time": 60, "dist": 65, "price": 3100},
    {"name": "å¤©ä¸‹æ£®", "url": "https://www.city.yokote.lg.jp/kanko/1004655/1004664/1001402.html", "lat": 39.2775, "lon": 140.5986, "time": 85, "dist": 95, "price": 2700},
    {"name": "å¤§æ›²ãƒ•ã‚¡ãƒŸãƒªãƒ¼", "url": "https://www.city.daisen.lg.jp/docs/2013110300234/", "lat": 39.4283, "lon": 140.5231, "time": 55, "dist": 60, "price": 2400},
    {"name": "ç¨²å·", "url": "https://www.city-yuzawa.jp/site/inakawaski/", "lat": 39.0681, "lon": 140.5894, "time": 95, "dist": 105, "price": 2500}
]

# --- API (å¤©æ°—) ---
@st.cache_data(ttl=3600)
def get_weather():
    res = {}
    for r in base_resorts:
        try:
            url = "https://api.open-meteo.com/v1/forecast"
            p = {"latitude": r["lat"], "longitude": r["lon"], "daily": "weathercode", "timezone": "Asia/Tokyo", "forecast_days": 2}
            d = requests.get(url, params=p, timeout=2).json()
            c1, c2 = d['daily']['weathercode'][0], d['daily']['weathercode'][1]
            w_map = {0:"â˜€ï¸", 1:"ğŸŒ¤ï¸", 2:"â˜ï¸", 3:"â˜ï¸", 45:"ğŸŒ«ï¸", 51:"ğŸŒ§ï¸", 53:"ğŸŒ§ï¸", 55:"ğŸŒ§ï¸", 61:"â˜”", 63:"â˜”", 71:"â˜ƒï¸", 73:"â˜ƒï¸", 75:"â˜ƒï¸", 77:"ğŸŒ¨ï¸", 80:"ğŸŒ¦ï¸", 85:"ğŸŒ¨ï¸", 95:"âš¡"}
            res[r["name"]] = {"t": w_map.get(c1, "-"), "tm": w_map.get(c2, "-")}
        except:
            res[r["name"]] = {"t": "-", "tm": "-"}
    return res

def fmt_time(m):
    return f"{m//60}æ™‚é–“{m%60}åˆ†" if m//60 > 0 else f"{m}åˆ†"

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
progress_bar = st.progress(0, text="ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹...")

# 1. å¤©æ°—
weather = get_weather()
progress_bar.progress(10, text="å¤©æ°—ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")

# 2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
df_list = []
cams = []
count = 0
total = len(base_resorts)

for i, r in enumerate(base_resorts):
    # é€²æ—æ›´æ–°
    progress_bar.progress(10 + int((i/total)*90), text=f"{r['name']} ã®ã‚µã‚¤ãƒˆè§£æä¸­...")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    scraped = scrape_resort(r['url'], r['name'])
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (æœªå–å¾—ã®å ´åˆã¯è¡¨ç¤ºã™ã‚‹è¨­å®š)
    is_open = "å–¶æ¥­" in scraped["status"] or "å¯" in scraped["status"]
    if filter_open_only and not is_open:
        continue
    
    count += 1
    w = weather.get(r["name"], {"t":"-", "tm":"-"})
    t_winter = int(r["time"] * 1.35)
    
    # è¡¨ç¤ºç”¨HTMLåŠ å·¥
    snow_disp = scraped["snow"]
    if snow_disp == "æœªå–å¾—":
        snow_disp = '<span class="no-data">æœªå–å¾—</span>'
    else:
        snow_disp = f"<b>{snow_disp}</b>"
        
    status_disp = scraped["status"]
    if "æœªå–å¾—" in status_disp or "ä¸æ˜" in status_disp:
        status_disp = '<span class="no-data">ä¸æ˜</span>'
    elif "â›”" in status_disp:
        status_disp = f'<span class="status-ng">{status_disp}</span>'
    else:
        status_disp = f'<span class="status-ok">{status_disp}</span>'

    df_list.append({
        "ã‚¹ã‚­ãƒ¼å ´": r["name"],
        "ç©é›ª": snow_disp,
        "çŠ¶æ³": status_disp,
        "ãƒªãƒ•ãƒˆåˆ¸": f"Â¥{r['price']:,}",
        f"å¤©æ°—({str_today})": w['t'],
        "è·é›¢/æ™‚é–“": f"{r['dist']}km/{fmt_time(t_winter)}",
        "ãƒªãƒ³ã‚¯": f'<a href="{r["url"]}" target="_blank" class="link-btn">å…¬å¼HP</a>',
        "lat": r["lat"], "lon": r["lon"], "raw_status": scraped["status"]
    })
    
    if r.get("live"):
        # ã‚«ãƒ¡ãƒ©ç”¨ãƒ‡ãƒ¼ã‚¿
        cam_item = r.copy()
        cam_item["status"] = scraped["status"]
        cams.append(cam_item)

progress_bar.empty()

# --- è¡¨ç¤º ---
st.success(f"ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº† ({ACCESS_TIME})")

if count == 0:
    st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ã‚¹ã‚­ãƒ¼å ´ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    df = pd.DataFrame(df_list)
    
    # 1. ä¸€è¦§è¡¨
    st.subheader("ğŸ“‹ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŠ¶æ³")
    html = df.drop(columns=["lat", "lon", "raw_status"]).to_html(classes="table", escape=False, index=False)
    st.markdown(f'<div class="table-container">{html}</div>', unsafe_allow_html=True)
    
    # 2. ã‚«ãƒ¡ãƒ©
    st.divider()
    st.subheader("ğŸ“· ãƒ©ã‚¤ãƒ–ã‚«ãƒ¡ãƒ©")
    cols_per_row = 3
    rows = [cams[i:i + cols_per_row] for i in range(0, len(cams), cols_per_row)]
    
    for row in rows:
        cols = st.columns(cols_per_row)
        for idx, cam in enumerate(row):
            with cols[idx]:
                if cam.get("yt_id"):
                    thumb = f"https://img.youtube.com/vi/{cam['yt_id']}/mqdefault.jpg"
                else:
                    bg = "008CBA" if "å–¶æ¥­" in cam['status'] or "å¯" in cam['status'] else "6c757d"
                    safe_name = cam['name'].replace(" ", "")
                    thumb = f"https://placehold.co/600x338/{bg}/FFFFFF/png?text={safe_name}"
                
                st.markdown(f"**{cam['name']}**")
                st.markdown(f"[![cam]({thumb})]({cam['live']})")

    # 3. ãƒãƒƒãƒ—
    st.divider()
    st.subheader("ğŸ—ºï¸ ãƒãƒƒãƒ—")
    m = folium.Map(location=[39.8, 140.5], zoom_start=9)
    for _, row in df.iterrows():
        color = "red" if "å–¶æ¥­" in row['raw_status'] or "å¯" in row['raw_status'] else "blue"
        folium.Marker(
            [row['lat'], row['lon']], 
            popup=row['ã‚¹ã‚­ãƒ¼å ´'], 
            icon=folium.Icon(color=color, icon="info-sign")
        ).add_to(m)
    st_folium(m, width="100%", height=500)
